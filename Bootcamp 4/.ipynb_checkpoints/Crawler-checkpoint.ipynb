{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A script that crawls the ventures of Sequoia Capital https://www.sequoiacap.com/companies\n",
    "\n",
    "Also used the following links for help\n",
    "#https://www.datacamp.com/community/tutorials/web-scraping-using-python\n",
    "#https://www.statworx.com/at/blog/web-scraping-101-in-python-with-requests-beautifulsoup/\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def delay() -> None:\n",
    "    time.sleep(random.uniform(15, 30))\n",
    "    return None\n",
    "\n",
    "\n",
    "base: str = \"https://www.sequoiacap.com/companies/\"\n",
    "content: dict = {\n",
    "    \"name\": [],\n",
    "    \"url\": [],\n",
    "    \"description\": [],\n",
    "}\n",
    "\n",
    "delay()\n",
    "r: requests.Response = requests.get(base)\n",
    "soup: bs4.BeautifulSoup = BeautifulSoup(r.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in soup.find_all(\n",
    "    \"div\", {\"class\": \"companies _company js-company\"}\n",
    "):\n",
    "    # Parse company name.\n",
    "    name = company.div.text\n",
    "\n",
    "    # Send request to the detail company page\n",
    "    # and parse it using BeautifulSoup.\n",
    "    r = requests.get(\"https://www.sequoiacap.com\" + company[\"data-url\"])\n",
    "    detailed_soup = BeautifulSoup(r.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse company url.\n",
    "url = detailed_soup.find_all(\"a\", {\"class\": \"social-link\"})[\"href\"]\n",
    "if len(url) == 0:\n",
    "    url = \"NA\"\n",
    "else:\n",
    "    url = detailed_soup.find(\"a\", {\"class\": \"social-link\"})[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse company description\n",
    "description = detailed_soup.find_all(\n",
    "    \"div\", {\"class\": \"company-holder _body-copy -grey-dark\"})\n",
    "if len(description) == 0:\n",
    "    description = \"NA\"\n",
    "else:\n",
    "    description = detailed_soup.find(\n",
    "        \"div\", {\"class\": \"company-holder _body-copy -grey-dark\"}\n",
    "    ).p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Append all data belonging to this company\n",
    "    # to the content dictionary.\n",
    "    content[\"name\"].append(name)\n",
    "    content[\"url\"].append(url)\n",
    "    content[\"description\"].append(description)\n",
    "    delay()\n",
    "\n",
    "# Write scraped data to disk.\n",
    "df: pd.DataFrame = pd.DataFrame(content)\n",
    "df = df.replace(\"\", np.nan).fillna(value=\"NA\")\n",
    "df.to_csv(\"../01_data/requests_output.csv\",\n",
    "          index=False,\n",
    "          sep=\";\",\n",
    "          encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
